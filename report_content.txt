
AirSign: Real-Time ASL Detection for Video ConferencingFinal Project ReportSubmitted By:Chloy Costa2447116 Elvis Elias Lobo2447129 Vismaya V2447158Under the guidance of Guide’s Name&gt;Guide’s Name Guide’s Designation&gt;Guide’s Designation2770427298277Department of Computer Science CHRIST (Deemed to be University), BangaloreIndiaAugust 2025ContentsExecutive Summary4Project Overview4Problem Statement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4Proposed Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4Project Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4System Architecture4Technology Stack. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4Frontend Technologies . . . . . . . . . . . . . . . . . . . . . . . . .5Backend Technologies . . . . . . . . . . . . . . . . . . . . . . . . .5AI/ML Components . . . . . . . . . . . . . . . . . . . . . . . . . .5System Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5Core Application (‘screen-sharing-app-multi‘) . . . . . . . . . . . . .5ASL Detection Service (‘model‘). . . . . . . . . . . . . . . . . . .6Data Flow Architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . .6Features6Core Video Conferencing Features . . . . . . . . . . . . . . . . . . . . . . .6ASL Detection System . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7Communication Features . . . . . . . . . . . . . . . . . . . . . . . . . . . .7Accessibility Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7How We Built It7WebRTC Setup. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7Setting Up Peer-to-Peer Calls . . . . . . . . . . . . . . . . . . . . .7ICE Candidate Handling . . . . . . . . . . . . . . . . . . . . . . . .8ASL Detection Integration . . . . . . . . . . . . . . . . . . . . . . . . . . .8Video Frame Capture. . . . . . . . . . . . . . . . . . . . . . . . .8API Communication . . . . . . . . . . . . . . . . . . . . . . . . . .8Real-Time Communication . . . . . . . . . . . . . . . . . . . . . . . . . . .8Database Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9UI/UX Design9Design Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9User Journey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9UI Screenshots10Challenges and Limitations12Technical Challenges12Scalability Limitations12User Experience Limitations12Future Work12Conclusion13Executive SummaryAirSign is a new video conferencing tool we built to help deaf and mute individuals communi- cate more easily. It uses real-time American Sign Language (ASL) detection during video calls. This project is a proof-of-concept to show that it’s possible to add AI-based sign language recognition into modern web-based meeting platforms. Our goal was to create a solution that makes online meetings more inclusive.We successfully combined a machine learning model for ASL detection with WebRTC, which is a technology for peer-to-peer video calls. In our tool, when a person uses sign language, their gestures are automatically recognized, and the corresponding word appears as a visual notification for everyone in the meeting.Project OverviewProblem StatementStandard video conferencing platforms like Zoom or Google Meet don’t have built-in features for people who use sign language. While they provide video, they lack any real-time sign language recognition. This makes it difficult for deaf and hearing people to communicate in the same meeting without hiring a human interpreter, which can be costly and inconvenient.Proposed SolutionOur project, AirSign, aims to solve this problem. It’s a video conferencing tool with real-time ASL detection that can:Recognize ASL gestures from a user’s video feed.Show the detected words as notifications to all participants in the call.Include all standard video conferencing features like audio, video, chat, and screen sharing.Provide a simple and accessible interface for everyone.Project ObjectivesPrimary Objective: To show that real-time ASL detection is feasible in a web-based video conferencing application.Secondary Objective: To design a user-friendly interface that works well for both deaf and hearing users.Technical Objective: To combine several technologies like WebRTC, AI/ML, and real-time communication (using Socket.IO) into a single, working platform.System ArchitectureTechnology StackWe used a combination of frontend, backend, and AI technologies to build AirSign.Frontend TechnologiesHTML5: For the basic structure of the web pages, with a focus on accessibility.CSS3: For styling the application, including responsive design and modern visuals.Vanilla JavaScript: For the main application logic and handling WebRTC connections.WebRTC: To enable direct peer-to-peer video and audio communication.Socket.IO: For real-time communication between the clients and the server (e.g., chat messages, ASL notifications).Font Awesome: For accessible icons used in the UI.Backend TechnologiesNode.js: As the server’s runtime environment.Express.js: A web application framework for Node.js to build the server logic.Socket.IO: For managing real-time, event-based communication on the server.MongoDB: A database to store user data and chat history.FastAPI (Python): To create the API service for the ASL detection model.AI/ML ComponentsRoboflow: A platform where we hosted our machine learning model for ASL detection.Computer Vision: Used to capture and analyze frames from the video streams.RESTful API: To allow the frontend to send video frames to the machine learning model and get results back.System ComponentsThe project is divided into two main parts: the core video conferencing application and the ASL detection service.Core Application (‘screen-sharing-app-multi‘)914400-34196public/js/client.js# Main application logicaslDetector.js # ASL detection system config.js# Configurationmeeting.html# Main meeting page premeeting.html# Pre-meeting setup page auth.html# Login/Signup pageserver.js# Backend serverpackage.json# Project dependenciespublic/js/client.js# Main application logicaslDetector.js # ASL detection system config.js# Configurationmeeting.html# Main meeting page premeeting.html# Pre-meeting setup page auth.html# Login/Signup pageserver.js# Backend serverpackage.json# Project dependencies12345678910Listing 1: Directory Structure of the Core ApplicationASL Detection Service (‘model‘)1app.py#FastAPI service2requirements.txt#Python dependencies3README.md#API documentationListing 2: Directory Structure of the ASL Detection ServiceData Flow ArchitectureThe ASL detection process works in a few simple steps:Video Capture: The user’s video is captured in the browser using WebRTC.Frame Extraction: Every few seconds, a single frame (an image) is taken from the video.ASL Processing: This frame is sent to our machine learning API to check for any ASL signs.Result Broadcasting: If a sign is detected, the resulting word is sent to all other participants in the meeting using Socket.IO.Visual Notification: The detected word appears in a small pop-up notification (a &quot;snackbar&quot;) on everyone’s screen.FeaturesCore Video Conferencing FeaturesMulti-Participant Support: Allows multiple users to join a single meeting room.Dynamic Layout: The video grid automatically adjusts based on the number of par- ticipants.Peer-to-Peer Connections: Uses WebRTC for direct and efficient communication between users.Modern UI: The user interface is inspired by Google Meet, with a focus on ease of use.Dark Theme: Reduces eye strain during long meetings.Intuitive Controls: Buttons are large, clearly labeled, and accessible via keyboard shortcuts.ASL Detection SystemReal-Time Recognition: The system detects ASL gestures from the live video feed.Visual Notifications: Detected words are shown in non-intrusive pop-ups.User Differentiation: The pop-up color is different for your own detections versus others’ detections, making it easy to see who is signing.Auto-Dismissal: Notifications disappear automatically after 2 seconds to avoid clutter.Configuration Management: Users can change the ASL API settings directly from the interface.Communication FeaturesReal-Time Chat: A standard chat feature for text-based communication. Chat history is saved in MongoDB.Participant Management: Users can see a list of who is currently in the meeting.Accessibility FeaturesInclusive Design: The interface uses high contrast and large buttons to be accessible for more people.Screen Reader Support: We used proper HTML and ARIA labels so the application works with screen readers.Keyboard Navigation: All features can be used with just a keyboard, no mouse required.Multiple Communication Modes: Users can communicate through video, audio, text chat, or sign language.How We Built ItWebRTC SetupWebRTC is used to create peer-to-peer connections for video and audio. This means the video data flows directly between users’ computers instead of going through our server, which reduces latency.Setting Up Peer-to-Peer CallsWe created a function to manage ‘RTCPeerConnection‘ objects. When a new user joins, a peer connection is created between them and every other user in the room.91440080290async function createPeerConnection(targetUserId, createOffer = false, userName) {const peer = new RTCPeerConnection(configuration); peers[targetUserId] = peer;async function createPeerConnection(targetUserId, createOffer = false, userName) {const peer = new RTCPeerConnection(configuration); peers[targetUserId] = peer;1239144008851localStream.getTracks().forEach(track =&gt; { peer.addTrack(track, localStream);});peer.ontrack = (event) =&gt; {if (event.streams &amp;&amp; event.streams[0]) { addRemoteVideo(targetUserId, event.streams[0], userName);}};return peer;}localStream.getTracks().forEach(track =&gt; { peer.addTrack(track, localStream);});peer.ontrack = (event) =&gt; {if (event.streams &amp;&amp; event.streams[0]) { addRemoteVideo(targetUserId, event.streams[0], userName);}};return peer;}456789101112131415Listing 3: Creating a WebRTC Peer ConnectionICE Candidate HandlingTo establish a connection, WebRTC uses ICE candidates to figure out the best way to connect two peers, especially if they are behind a firewall. We used Google’s public STUN servers to help with this process.ASL Detection IntegrationThis is the core feature of our project. We periodically capture frames from the user’s video and send them to our AI model.Video Frame CaptureWe wrote a JavaScript function to grab a frame from a video element and convert it into a format that can be sent over the network.91440080206function captureFrame(video) {const canvas = document.createElement(’canvas’); const ctx = canvas.getContext(’2d’); canvas.width = video.videoWidth;canvas.height = video.videoHeight; ctx.drawImage(video, 0, 0); return canvas.toBlob();}function captureFrame(video) {const canvas = document.createElement(’canvas’); const ctx = canvas.getContext(’2d’); canvas.width = video.videoWidth;canvas.height = video.videoHeight; ctx.drawImage(video, 0, 0); return canvas.toBlob();}12345678Listing 4: Capturing a Frame from a Video ElementAPI CommunicationThe captured frame is sent to our FastAPI service using an HTTP POST request. We added error handling and limited the detection rate to every 3-5 seconds to avoid overloading the system.Real-Time CommunicationWe used Socket.IO to manage real-time events like chat messages and ASL detection results. When the server receives a detected word, it broadcasts it to all other users in the same meeting room.91440046951socket.on(’asl-detection’, (data) =&gt; { if (data.sender !== username) {showASLSnackbar(data.word, false, data.sender);}});socket.on(’asl-detection’, (data) =&gt; { if (data.sender !== username) {showASLSnackbar(data.word, false, data.sender);}});12345Listing 5: Receiving ASL Detection from Another UserDatabase IntegrationWe used MongoDB to store user accounts and chat history. We created simple schemas for users and chat messages. The application also has a demo mode that works even if a database is not connected.UI/UX  DesignDesign PrinciplesAccessibility First: We designed the interface to be easy to use for everyone, with high contrast colors, a clean layout, and large buttons.Modern Look: The design is simple and modern, with a gradient background and a clean font.User JourneyPre-Meeting: Users can sign up, log in, or use the demo mode. They can create a new meeting with one click or join an existing one with a code.In-Meeting: The interface includes a video grid, a control bar at the bottom, and a slide-out panel for chat. ASL detection can be turned on or off with a single click.Post-Meeting: When a user leaves, all connections are properly closed. The chat history is saved for later reference.UI Screenshots1176210253701Figure 1: Welcome Page1176210218516Figure 2: Login PageFigure 3: User Dashboard for Starting or Joining a Meeting1178737220603meeting_interface.pngmeeting_interface.pngFigure 4: Main Meeting Interface with ASL Detection Pop-upChallenges and LimitationsTechnical ChallengesASL Detection Accuracy: The performance of the AI model was very sensitive to lighting conditions and video quality. Also, our model only recognizes a limited number of basic ASL signs.WebRTC Complexity: Setting up peer-to-peer connections can be tricky due to network firewalls (NAT traversal). We also found small differences in how browsers like Chrome and Safari support WebRTC.Scalability LimitationsParticipant Count: The application works best with 2-6 participants. More users might cause performance issues.Server Resources: The ASL detection API can use a lot of CPU, which could be a bottleneck if many users are using it at once.User Experience LimitationsDetection Delay: There is a 2-3 second delay between making a sign and seeing the notification, which can make conversations feel less natural.Limited Vocabulary: Since our model only knows a few signs, it limits how much users can communicate with it.No Grammar Support: The system detects single words, not the full grammatical structure of ASL.Future WorkThere are many ways this project could be improved in the future:Improve ASL Detection: We could train a custom machine learning model with a much larger vocabulary (500+ words) and even try to recognize full sentences. We could also explore running the model directly in the browser to reduce latency.Add More Features: We could add features like meeting recording, virtual back- grounds, and file sharing in the chat.Enhance Accessibility: We could add text-to-speech for chat messages and support for other sign languages like British Sign Language (BSL).Improve Architecture: For a larger-scale application, we could move to a microser- vices architecture to make the system more scalable and easier to maintain.ConclusionOur project, AirSign, successfully demonstrates that it is possible to integrate real-time ASL detection into a web-based video conferencing tool. We built a working proof-of-concept that combines WebRTC, machine learning, and real-time messaging into a single, user-friendly platform.This project shows how technology can be used to break down communication barriers and make the digital world more inclusive. While there are limitations, particularly with the accuracy and vocabulary of the ASL model, it serves as a strong foundation for future development. The code is open, the concept is proven, and we believe this type of technology has immense potential to positively impact the deaf and hard-of-hearing community. This project is a starting point for building better and more accessible communication tools for everyone.